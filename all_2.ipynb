{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import re\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.9\n",
    "\n",
    "# model params\n",
    "batch_size = 128 # b, to be changed\n",
    "sequence_l = 128 # n\n",
    "d_model = 768 # d_modelï¼Œ embedding dim\n",
    "num_layer = 12 # number of blocks stacked\n",
    "number_head = 8 # multihead attention\n",
    "d_ff = 2048 # feedforward dimension\n",
    "dropout = 0.2\n",
    "learning_rate = 1e-4\n",
    "max_epoch = 3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,sequence_l):\n",
    "        self.data = self.load_data()\n",
    "        chars = sorted(list(set(self.data)))\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.stoi) }\n",
    "        self.sequence_l = sequence_l\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_l\n",
    "    \n",
    "    def load_data(self):\n",
    "        with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.sequence_l + 1]\n",
    "        # encode every character to an integer\n",
    "        idx_chunk = [self.stoi[c] for c in chunk]\n",
    "        x = torch.tensor(idx_chunk[:-1], dtype=torch.long)\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        y = torch.tensor(idx_chunk[1:], dtype=torch.long)\n",
    "        return x,y\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "dataset = CustomDataset(sequence_l)\n",
    "\n",
    "# Create a data loader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        # pe: [seq_lens * 1 * d_model] for each sample\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(d_model,head_size,bias=False)\n",
    "        self.query = nn.Linear(d_model,head_size,bias=False)\n",
    "        self.value = nn.Linear(d_model,head_size,bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(sequence_l, sequence_l)))\n",
    "\n",
    "\n",
    "    def forward(self,x): # x:[batch, l_seq, d_model]\n",
    "        k = self.key(x) # k:[batch, l_seq, head_size]\n",
    "        q = self.query(x) # q:[batch, l_seq, head_size]\n",
    "        v = self.value(x) # v:[batch, l_seq, head_size]\n",
    "        qkt = q@k.transpose(2,1)/self.head_size**0.5 #[batch*l_seq*l_seq]  \n",
    "        qkt = qkt.masked_fill(self.tril == 0, float('-inf'))\n",
    "        qkt = F.softmax(qkt, dim = -1)\n",
    "        z = qkt@v # z:[batch * l_seq*l_seq]@[batch, l_seq, head_size] = [batch, l_seq, head_size]\n",
    "        return z\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,number_head,head_size):\n",
    "        super().__init__()\n",
    "        self.self_attention = nn.ModuleList([Head(head_size) for _ in range(number_head)])\n",
    "        self.w0 = nn.Linear(head_size*number_head,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        head_outputs = [head(x) for head in self.self_attention]\n",
    "        output = torch.cat(head_outputs, dim=-1) # [batch, l_seq, head_size*number_head]\n",
    "        output = self.w0(output) # output:[batch, l_seq, d_model], so that it can be added with residual \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Multi-Heads Self-Attention mechanism is followed by two fully connected layers of \n",
    "# the Feed Forward block. The first (hidden) layer contains 4 times as many neurons as the input \n",
    "# sequence with the ReLU activation function. The dimension of the second layer is \n",
    "# equal to the dimension of the input sequence, and neurons do not use the activation function.\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model):\n",
    "        super().__init__()\n",
    "        self.ff=nn.Sequential( nn.Linear(d_model,4*d_model),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(4*d_model,d_model))\n",
    "    def forward(self,x):\n",
    "        x = self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = d_model // number_head\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(number_head,head_size)\n",
    "        \n",
    "        self.norm1  = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = FeedForward(d_model)\n",
    "        \n",
    "        self.norm2  = nn.LayerNorm(d_model)###\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.self_attention(self.norm1(x))\n",
    "        out = x + self.ffn(self.norm2(x))\n",
    "         \n",
    "        return out    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,stoi):\n",
    "        super().__init__()\n",
    "        self.stoi = stoi\n",
    "        self.tok_emb = nn.Embedding(len(stoi),d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(num_layer)])\n",
    "        self.norm_final = nn.LayerNorm(d_model)\n",
    "        self.predict = nn.Linear(d_model,len(stoi))\n",
    "        self.loss_compute = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, use='train',y = None ):\n",
    "        emb_x = self.tok_emb(x)\n",
    "        emb_x = self.pos_emb(emb_x) # x,y = emb = [batch size * sequence_l * d_model]\n",
    "        emb_x = self.dropout1(emb_x)\n",
    "\n",
    "        emb_x = self.blocks(emb_x)\n",
    "        x = self.norm_final(emb_x)\n",
    "        logit = self.predict(x) #[batch size * sequence_l * number_of_char]\n",
    "        # y:[batch size * l_sequence * 1]\n",
    "\n",
    "        if use == 'train':\n",
    "            logit = logit.view(batch_size*sequence_l,len(self.stoi))\n",
    "            y = y.view(batch_size*sequence_l)\n",
    "            loss = self.loss_compute(logit,y)\n",
    "        elif use == 'generate':\n",
    "            loss = None\n",
    "\n",
    "        return logit, loss # loss for training, logit for generate       \n",
    "    \n",
    "    def generate(self, output_length, seed_idx):\n",
    "        out = seed_idx\n",
    "        for i in range(output_length):\n",
    "            print(i)\n",
    "            logit,_ = self(seed_idx, use = 'generate')\n",
    "            prob = F.softmax(logit,dim = -1)\n",
    "            next_idx = prob[-1,-1,:].argmax()\n",
    "            out = torch.cat([out , next_idx.unsqueeze(0)], dim=-1)\n",
    "            seed_idx = out[-sequence_l:]\n",
    "        return out\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(stoi=dataset.stoi)\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1):\n",
    "    # Iterate through the data loader\n",
    "    for batch in data_loader:\n",
    "        # Process each batch of data here\n",
    "        x, y = batch\n",
    "        # Your model training code goes here\n",
    "\n",
    "        logit,loss = m(x,'train', y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = dataset.stoi\n",
    "itos = dataset.itos\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\all_2.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_2.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m seed \u001b[39m=\u001b[39m text[:\u001b[39m10\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_2.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m empty \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39msequence_l\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_2.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m seed_idx \u001b[39m=\u001b[39m encode(seed)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "seed = text[:10]\n",
    "empty = \" \"*sequence_l\n",
    "seed_idx = encode(seed)\n",
    "if len(seed)<sequence_l:\n",
    "    input_idx = encode(empty)\n",
    "    input_idx[-len(seed):] = seed_idx\n",
    "else:\n",
    "    input_idx = seed_idx[:sequence_l]\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_idx = torch.tensor(input_idx,dtype=torch.long).to(device)\n",
    "\n",
    "    generated = m.generate(3,input_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                      First Citifhl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(generated.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seed_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\all_2.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_2.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m logit \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m128\u001b[39m,\u001b[39m128\u001b[39m,\u001b[39m65\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_2.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m seed_idx\n",
      "\u001b[1;31mNameError\u001b[0m: name 'seed_idx' is not defined"
     ]
    }
   ],
   "source": [
    "logit = torch.randn((128,128,65))\n",
    "seed_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\all_2.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_2.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prob \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logit,dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_2.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m next_idx \u001b[39m=\u001b[39m prob[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\u001b[39m.\u001b[39margmax()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_2.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mtensor(input_idx) , next_idx\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_idx' is not defined"
     ]
    }
   ],
   "source": [
    "prob = F.softmax(logit,dim = -1)\n",
    "next_idx = prob[-1,-1,:].argmax()\n",
    "\n",
    "torch.cat([torch.tensor(input_idx) , next_idx.unsqueeze(0)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(seed_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
