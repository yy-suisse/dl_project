{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xSf-blsN4EcU"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import re\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrHtPTXY4EcY"
      },
      "source": [
        "# define global variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Q9m_JM64Eca"
      },
      "outputs": [],
      "source": [
        "split_ratio = 0.9\n",
        "\n",
        "# model params\n",
        "batch_size = 128 # b, to be changed\n",
        "sequence_l = 128 # n\n",
        "d_model = 768 # d_modelï¼Œ embedding dim\n",
        "num_layer = 12 # number of blocks stacked\n",
        "number_head = 8 # multihead attention\n",
        "d_ff = 2048 # feedforward dimension\n",
        "dropout = 0.2\n",
        "learning_rate = 1e-4\n",
        "max_epoch = 3\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "with open('data.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4YHHjFl4Ecb"
      },
      "source": [
        "# define dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M3d8nE2e4Ecc"
      },
      "outputs": [],
      "source": [
        "# Define your custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,sequence_l,device):\n",
        "        self.data = self.load_data()\n",
        "        chars = sorted(list(set(self.data)))\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(self.stoi) }\n",
        "        self.sequence_l = sequence_l\n",
        "        self.device = device\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.stoi)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.sequence_l\n",
        "\n",
        "    def load_data(self):\n",
        "        with open('data.txt', 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "        return text\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.sequence_l + 1]\n",
        "        # encode every character to an integer\n",
        "        idx_chunk = [self.stoi[c] for c in chunk]\n",
        "        x = torch.tensor(idx_chunk[:-1], dtype=torch.long)\n",
        "        # return the chunk and the shifted version as tensors\n",
        "        y = torch.tensor(idx_chunk[1:], dtype=torch.long)\n",
        "        x,y = x.to(self.device),y.to(self.device)\n",
        "\n",
        "        return x,y\n",
        "\n",
        "\n",
        "\n",
        "# Create an instance of your custom dataset\n",
        "dataset = CustomDataset(sequence_l,device)\n",
        "\n",
        "# Create a data loader\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNKwKp9b4Ecc"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UHEZm00j4Ecd"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        # pe: [seq_lens * 1 * d_model] for each sample\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XDZCsDXh4Ece"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.key = nn.Linear(d_model,head_size,bias=False)\n",
        "        self.query = nn.Linear(d_model,head_size,bias=False)\n",
        "        self.value = nn.Linear(d_model,head_size,bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(sequence_l, sequence_l)))\n",
        "\n",
        "\n",
        "    def forward(self,x): # x:[batch, l_seq, d_model]\n",
        "        k = self.key(x) # k:[batch, l_seq, head_size]\n",
        "        q = self.query(x) # q:[batch, l_seq, head_size]\n",
        "        v = self.value(x) # v:[batch, l_seq, head_size]\n",
        "        qkt = q@k.transpose(2,1)/self.head_size**0.5 #[batch*l_seq*l_seq]\n",
        "        qkt = qkt.masked_fill(self.tril == 0, float('-inf'))\n",
        "        qkt = F.softmax(qkt, dim = -1)\n",
        "        z = qkt@v # z:[batch * l_seq*l_seq]@[batch, l_seq, head_size] = [batch, l_seq, head_size]\n",
        "        return z\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FM0tDqYD4Ece"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,number_head,head_size):\n",
        "        super().__init__()\n",
        "        self.self_attention = nn.ModuleList([Head(head_size) for _ in range(number_head)])\n",
        "        self.w0 = nn.Linear(head_size*number_head,d_model)\n",
        "\n",
        "    def forward(self,x):\n",
        "        head_outputs = [head(x) for head in self.self_attention]\n",
        "        output = torch.cat(head_outputs, dim=-1) # [batch, l_seq, head_size*number_head]\n",
        "        output = self.w0(output) # output:[batch, l_seq, d_model], so that it can be added with residual\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gJ0QeUsB4Ecf"
      },
      "outputs": [],
      "source": [
        "# The Multi-Heads Self-Attention mechanism is followed by two fully connected layers of\n",
        "# the Feed Forward block. The first (hidden) layer contains 4 times as many neurons as the input\n",
        "# sequence with the ReLU activation function. The dimension of the second layer is\n",
        "# equal to the dimension of the input sequence, and neurons do not use the activation function.\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self,d_model):\n",
        "        super().__init__()\n",
        "        self.ff=nn.Sequential( nn.Linear(d_model,4*d_model),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Linear(4*d_model,d_model))\n",
        "    def forward(self,x):\n",
        "        x = self.ff(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gopZbqmW4Ecf"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        head_size = d_model // number_head\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(number_head,head_size)\n",
        "\n",
        "        self.norm1  = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ffn = FeedForward(d_model)\n",
        "\n",
        "        self.norm2  = nn.LayerNorm(d_model)###\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x + self.self_attention(self.norm1(x))\n",
        "        out = x + self.ffn(self.norm2(x))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WVmJLOTR4Ecg"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self,stoi):\n",
        "        super().__init__()\n",
        "        self.stoi = stoi\n",
        "        self.tok_emb = nn.Embedding(len(stoi),d_model)\n",
        "        self.pos_emb = PositionalEncoding(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(num_layer)])\n",
        "        self.norm_final = nn.LayerNorm(d_model)\n",
        "        self.predict = nn.Linear(d_model,len(stoi))\n",
        "        self.loss_compute = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x, use='train',y = None ):\n",
        "        emb_x = self.tok_emb(x)\n",
        "        emb_x = self.pos_emb(emb_x) # x,y = emb = [batch size * sequence_l * d_model]\n",
        "        emb_x = self.dropout1(emb_x)\n",
        "\n",
        "        emb_x = self.blocks(emb_x)\n",
        "        x = self.norm_final(emb_x)\n",
        "        logit = self.predict(x) #[batch size * sequence_l * number_of_char]\n",
        "        # y:[batch size * l_sequence * 1]\n",
        "\n",
        "        if use == 'train':\n",
        "            logit = logit.view(batch_size*sequence_l,len(self.stoi))\n",
        "            y = y.view(batch_size*sequence_l)\n",
        "            loss = self.loss_compute(logit,y)\n",
        "        elif use == 'generate':\n",
        "            loss = None\n",
        "\n",
        "        return logit, loss # loss for training, logit for generate\n",
        "\n",
        "    def generate(self, output_length, seed_idx, criteria):\n",
        "        out = seed_idx\n",
        "        for _ in range(output_length):\n",
        "            logit,_ = self(seed_idx, use = 'generate')\n",
        "            prob = F.softmax(logit[-1,-1,:], dim = -1)\n",
        "            \n",
        "            if criteria == 'high_prob': \n",
        "                next_idx = prob.argmax()\n",
        "                out  = torch.cat([out , next_idx.unsqueeze(0)], dim=-1)\n",
        "            else: \n",
        "                next_idx = torch.multinomial(prob, 1)\n",
        "                out  = torch.cat([out , next_idx], dim=-1)\n",
        "            \n",
        "            seed_idx = out[-sequence_l:]\n",
        "        return out\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0O1VM1Q4Ecg"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model(stoi=dataset.stoi)\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vpPcQNFN4Ecg",
        "outputId": "912ee525-dccf-47cc-9d7c-4fb586169362"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\all_3.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m x, y \u001b[39m=\u001b[39m batch\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Your model training code goes here\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m logit,loss \u001b[39m=\u001b[39m m(x,\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\all_3.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m emb_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_emb(emb_x) \u001b[39m# x,y = emb = [batch size * sequence_l * d_model]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m emb_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(emb_x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m emb_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(emb_x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_final(emb_x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m logit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(x) \u001b[39m#[batch size * sequence_l * number_of_char]\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\all_3.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     out \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mffn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\all_3.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/all_3.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(20):\n",
        "    # Iterate through the data loader\n",
        "    for batch in data_loader:\n",
        "        # Process each batch of data here\n",
        "        x, y = batch\n",
        "        # Your model training code goes here\n",
        "        logit,loss = m(x,'train', y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjCz0sNV_TLE"
      },
      "source": [
        "# result generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HlWZybGX_iBN"
      },
      "outputs": [],
      "source": [
        "stoi = dataset.stoi\n",
        "itos = dataset.itos\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "0P6ZDOSh_Sjj",
        "outputId": "0a1e5ee2-799a-4eeb-c33f-f3f550136314"
      },
      "outputs": [],
      "source": [
        "seed = text[:10]\n",
        "empty = \" \"*sequence_l\n",
        "seed_idx = encode(seed)\n",
        "if len(seed)<sequence_l:\n",
        "    input_idx = encode(empty)\n",
        "    input_idx[-len(seed):] = seed_idx\n",
        "else:\n",
        "    input_idx = seed_idx[:sequence_l]\n",
        "\n",
        "input_idx = torch.tensor(input_idx,dtype=torch.long).to(device)\n",
        "\n",
        "out = m.generate(5,input_idx,'high_prob')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80TAItJu_zUa",
        "outputId": "f102b3f8-aaca-489d-df1e-cf1b6d95774d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\yy\\AppData\\Local\\Temp\\ipykernel_21292\\3211163359.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  hi = F.softmax(hi)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([54])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hi = torch.randn((65,))\n",
        "hi = F.softmax(hi)\n",
        "sample_index = torch.multinomial(hi, 1)\n",
        "sample_index\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
