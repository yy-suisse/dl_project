{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen: before we proceed any further hear me speak all: speak speak first citizen: you are a\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    batch_size = 50 # b\n",
    "    sequence_l = 128 # n\n",
    "    d_model = 768 # d_modelï¼Œ embedding dim\n",
    "    num_layer = 12 # number of block stacked\n",
    "    number_head = 8 # multihead attention\n",
    "    d_ff = 2048 # feedforward dimension\n",
    "\n",
    "config_model = Config()\n",
    "\n",
    "def read_data(): \n",
    "    text = open('data.txt', 'r').read()\n",
    "    return re.sub('[^A-Za-z:]+',' ',text).strip().lower()\n",
    "\n",
    "data = read_data()\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        chars = sorted(list(set(data)))  # get set of characters from the input data\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}  # map characters to integer indices\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "        self.block_size = config.sequence_l\n",
    "        self.data = data\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size #possible index for beginning of batch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        idx_chunk = [self.stoi[c] for c in chunk]\n",
    "        x = torch.tensor(idx_chunk[:-1], dtype=torch.long)\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        y = torch.tensor(idx_chunk[1:], dtype=torch.long)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "char_dataset = CharDataset(config_model, data)\n",
    "# Create a DataLoader\n",
    "data_loader = DataLoader(char_dataset, batch_size=config_model.batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just take first batch for testing embedding\n",
    "for ind,(x,y) in enumerate(data_loader):\n",
    "    if ind != 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        # pe: [seq_lens * 1 * d_model] for each sample\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(config_model):\n",
    "    mask = torch.full([config_model.sequence_l, config_model.sequence_l] , float('-inf'))\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1361256923.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    class LayerNormalization(nn.Module):\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class LayerNormalization(nn.Module):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads,d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = d_model // num_heads\n",
    "\n",
    "        self.key = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.query = nn.Linear(d_model, head_size, bias=False)\n",
    "        self.value = nn.Linear(d_model, head_size, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.key(x) # x:[batch*l_seq*d_model] -> k:[batch * l_seq * head_size]\n",
    "        q = self.query(x) # x:[batch*l_seq*d_model] -> q:[batch * l_seq * head_size]\n",
    "        print(k.shape)\n",
    "        print(q.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderBlock, self).__init__()   \n",
    "        self.self_attention =  SingleHeadAttention(d_model,num_heads)\n",
    "        \"\"\"\n",
    "        self.norm1 = LayerNormalization()\n",
    "        self.ffn \n",
    "        self.norm2 = LayerNormalization()\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self,x,y,mask):\n",
    "        _x = x # used for skip connection\n",
    "        x = self.self_attention(x,mask) \n",
    "        \"\"\"\n",
    "        x = self.norm1(x+_x)\n",
    "\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.norm2(x+_x)\n",
    "     \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Sequential): # just un intermediate function for calling\n",
    "    def forward(self, x,y,mask):\n",
    "        for module in self._modules.values():\n",
    "            y = module(x,y,mask)\n",
    "            return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask =  create_causal_mask(config_model) #[sequence_l * sequence_l]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, src_vovab_size, ffn_hidden, num_heads, drop_prob, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(src_vovab_size,d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model)\n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.layers = SequentialDecoder(*[DecoderBlock(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x,y,mask):\n",
    "\n",
    "        emb_x = self.tok_emb(x)\n",
    "        emb_x = self.pos_emb(emb_x) # x,y = emb = [batch size * sequence_l * d_model]\n",
    "        emb_x = self.dropout1(emb_x)\n",
    "        return emb_x\n",
    "        y = self.layers(emb_x,y,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LayerNormalization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\model.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m src_vovab_size \u001b[39m=\u001b[39m char_dataset\u001b[39m.\u001b[39mget_vocab_size()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dec \u001b[39m=\u001b[39m Decoder(config_model\u001b[39m.\u001b[39;49md_model,src_vovab_size,config_model\u001b[39m.\u001b[39;49md_ff,config_model\u001b[39m.\u001b[39;49mnumber_head,\u001b[39m0.5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m hi \u001b[39m=\u001b[39m SingleHeadAttention(\u001b[39m8\u001b[39m,config_model\u001b[39m.\u001b[39md_model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m x_emb \u001b[39m=\u001b[39m dec(x)\n",
      "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\model.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_emb \u001b[39m=\u001b[39m PositionalEncoding(d_model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(drop_prob)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m SequentialDecoder(\u001b[39m*\u001b[39m[DecoderBlock(d_model, ffn_hidden, num_heads, drop_prob) \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(num_layers)])\n",
      "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\model.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_emb \u001b[39m=\u001b[39m PositionalEncoding(d_model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(drop_prob)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m SequentialDecoder(\u001b[39m*\u001b[39m[DecoderBlock(d_model, ffn_hidden, num_heads, drop_prob) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_layers)])\n",
      "\u001b[1;32mc:\\Users\\yy\\Desktop\\dl_project\\model.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39msuper\u001b[39m(DecoderBlock, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()   \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention \u001b[39m=\u001b[39m  SingleHeadAttention(d_model,num_heads)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1 \u001b[39m=\u001b[39m LayerNormalization()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yy/Desktop/dl_project/model.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2 \u001b[39m=\u001b[39m LayerNormalization()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LayerNormalization' is not defined"
     ]
    }
   ],
   "source": [
    "src_vovab_size = char_dataset.get_vocab_size()\n",
    "\n",
    "dec = Decoder(config_model.d_model,src_vovab_size,config_model.d_ff,config_model.number_head,0.5)\n",
    "hi = SingleHeadAttention(8,config_model.d_model)\n",
    "\n",
    "x_emb = dec(x)\n",
    "hi(x_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module): \n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
